---
title: "Deep reinforcement learning-based task offloading and resource allocation for industrial IoT in MEC federation system"
collection: publications
category: manuscripts
permalink: /publication/number-1
excerpt: 'Keyword: <em>MEC, Optimization, Energy and Delay tradeoff, DRL<em>'
date: 2023-08-07
venue: 'IEEE Access'
slidesurl: 
paperurl: 'http://academicpages.github.io/files/paper1.pdf'
citation: '<em>Do, Huong Mai, Tuan Phong Tran, and Myungsik Yoo. "Deep reinforcement learning-based task offloading and resource allocation for industrial IoT in MEC federation system." IEEE Access (2023).<em>'
---

**ABSTRACT** The rapid growth of the Internet of Things (IoT) has resulted in the development of intelligent industrial systems known as Industrial IoT (IIoT). These systems integrate smart devices, sensors, cameras, and 5G technologies to enable automated data gathering and analysis boost production efficiency and overcome scalability issues. However, IoT devices have limited computer power, memory, and battery capacities. To address these challenges, mobile edge computing (MEC) has been introduced to IIoT systems to reduce the computational burden on the devices. While the dedicated MEC paradigm limits optimal resource utilization and load balancing, the MEC federation can potentially overcome these drawbacks. However, previous studies have relied on idealized assumptions when developing optimal models, raising concerns about their practical applicability. In this study, we investigated the joint decision offloading and resource allocation problem for MEC federation in the IIoT. Specifically, an optimization model was con- structed based on all real-world factors influencing system performance. To minimize the total energy delay cost, the original problem was transformed into a Markov decision process. Considering task generation dynamics and continuity, we addressed the Markov decision process using a deep reinforcement learning method. We propose a deep deterministic policy gradient algorithm with prioritized experience replay (DDPG-PER)-based resource allocation that can handle high-dimensional continuity of action and state spaces. The simulation results indicate that the proposed approach effectively minimizes the energy-delay costs associated with tasks.